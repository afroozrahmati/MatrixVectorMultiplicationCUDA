# MatrixVectorMultiplicationCUDA
Matrix Vector Multiplication using CUDA
 

GEMV on GPU: Memory and Profiler 

 

This is a quick lab.   The goal is to explore the use of different memory spaces in GPU and introduce the use of profiler Nvidia Compute (given that you are using CUDA v10 and above) 

 

 

Before starting, select 5 interesting and valid execution configuration choices (think about your findings in Lab 2).   In your report, you must explain your criteria for those choices.    In Part 0 below, you will collect metrics observed with the naïve GEMV and use those as the "baseline". 

 

You must use vectors of different sizes, i.e., not necessarily sizes multiple of 32.    

 

For each part below, collect and report FLOPS, Cache transactions (e.g., l1_cache_global_hit_rate, l1_cache_local_hit_rate), Shared memory, and number of registers used per thread. 

 

Hints:  revisit loop unrolling, and also recall our in-class discussion for basic blocked vector addition, where we talked about having partially empty blocks vs. only one block with all the idle threads (for the scenario when the vector is not of size multiple of 32bits) 

 

NOTE: The tasks you are required to do below,  won't necessarily result in optimizations!  The purpose of this lab is to exercise different strategies and use the profiler to determine whether they can improve performance or not. 

 

If you are running on Linux and you have issues generating the log file generated by the profiler, check the folder where it is going to by default.  Check the permissions of such folder.  If you don't have permissions to write to it, immediately email our IT and cc me. 

IT emails: sinchai@uw  and regomez@uw 

 

Part 0 

Do a naïve GEMV (baseline) 

Profile it AS IS 

 

Part 1 

Try a modification of the naïve version related to global memory use. This one is easy, start by just trying out 5 additional  configurations slightly different to the initial 5 you picked,  try configurations on sizes multiples and non-multiples of 32). Remember that changes related to global memory use have to do with thread management.  Review "Global Memory considerations" 

Run and profile 

Report results of one or two runs where you observed interesting changes and explain the WHY of these differences 

 
Part 2  

Try a modification of the naïve version related to shared memory use:  This one might be a little trickier.   Recall that for shared memory we take advantage of locality.    Discuss in your report if and how you did this modification, and if not, why.   E.g.,  could you try out very large vectors being processed by just a few blocks, where the block size is not a multiple of 32  and use partially empty blocks? 

Run and Profile  

Report results of one or two runs where you observed interesting changes and explain the WHY of these differences 

 

Part 3  

Try a modification of the naïve version related to registers  (try loop unrolling and inspect the number of registers per thread). Then try to overflow the number of registers available and see how this may (or not) impact your performance since there will be overflow to Local Memory.   Note that you would have to use very large vectors. 

Run and Profile  

Report results of one or two runs where you observed interesting changes and explain the WHY of these differences 

 

 

Deliverables 

Code  

Submit one .zip file or one .cu file (You may have 4 different .cu files or have all your tests in one) 

 

Report   

Submit a docx or pdf file separately from the code 

As usual, don't forget to include hardware specifications of the GPU that you are using as well as  the compute capability and Cuda version. 

Discuss the selection of execution configurations 

Discuss your experiment setup, e.g., what changes did you make to the original vector addition algorithm in each of the parts. 

Include your results for each of your experiments 

Analyze and discuss your results 
